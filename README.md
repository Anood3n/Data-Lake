
# Data Lake Project

## Introduction 
This project aims to create ETL pipeline that extracts Sparkify streaming app data from Amazon S3 buckets,process the data using Apach Spark then load the data back into s3. This will allow Sparkify analytics team to continue finding insights in understanding which songs users listen to the most in the app.


## DataSets   
Sparkify app data resides in two public Amazon S3 buckets:

1- Song Dataset: (all files are in one directory in a JASON format)
    - Song data: 's3://udacity-dend/song_data'
    - a subset of real data from the Million Song Dataset 
    - contains the metadata on the song on the app and the artist of the song 
    

2- Log Dataset:(files are distributed  into several folders several folders without a common prefix, so we will use a descriptor file called "Log data json path" to extract them )
    - Log data: 's3://udacity-dend/log_data'
    - Log data json path: 's3://udacity-dend/log_json_path.json'
    - generated by event simulator based on the songs in the Million Song Dataset 
    - contains the logs on user activity in any given day   

## Project repository
we have one python file and a configration file :
    
1- "etl.py", contains the extraction and the loading of data from the S3 bucket,processes that data using Spark, and ingest them back to S3.

2- "dhw.cfg", it's a configuration file that contains the needed information about AWS credentials. 


